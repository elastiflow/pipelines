# Benchmark Testing

## Benchmark Structure

### Basic Benchmark
```go
func BenchmarkFunctionName(b *testing.B) {
    // Setup
    input := generateTestData(1000)
    
    b.ResetTimer()
    b.ReportAllocs()
    
    for i := 0; i < b.N; i++ {
        FunctionName(input)
    }
}
```

### Pipeline Benchmark
```go
func BenchmarkPipeline_Throughput(b *testing.B) {
    // Setup context and error handling
    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
    defer cancel()
    
    errChan := make(chan error, 100)
    defer close(errChan)
    
    // Generate test data
    testData := generateTestData(b.N)
    source := sources.FromArray(testData)
    
    b.ResetTimer()
    b.ReportAllocs()
    
    // Run pipeline
    pipeline := pipelines.New[Data, Result](ctx, source, errChan)
    pipeline.Start(processingFunc)
    
    // Consume results
    for range pipeline.Out() {
        // Process results
    }
}
```

## Performance Metrics

### Memory Allocation
```go
func BenchmarkMemoryUsage(b *testing.B) {
    var m runtime.MemStats
    runtime.ReadMemStats(&m)
    before := m.Alloc
    
    b.ResetTimer()
    b.ReportAllocs()
    
    for i := 0; i < b.N; i++ {
        // Operation to benchmark
    }
    
    runtime.ReadMemStats(&m)
    after := m.Alloc
    
    b.ReportMetric(float64(after-before)/float64(b.N), "bytes/op")
}
```

### Throughput Measurement
```go
func BenchmarkThroughput(b *testing.B) {
    // Setup with fixed time
    duration := 5 * time.Second
    ctx, cancel := context.WithTimeout(context.Background(), duration)
    defer cancel()
    
    // Count processed items
    var processed int64
    
    b.ResetTimer()
    
    // Run until context expires
    for ctx.Err() == nil {
        // Process item
        atomic.AddInt64(&processed, 1)
    }
    
    // Report throughput
    throughput := float64(processed) / duration.Seconds()
    b.ReportMetric(throughput, "items/sec")
}
```

## Benchmark Variants

### Different Data Sizes
```go
func BenchmarkFunctionName_DataSizes(b *testing.B) {
    sizes := []int{10, 100, 1000, 10000}
    
    for _, size := range sizes {
        b.Run(fmt.Sprintf("Size_%d", size), func(b *testing.B) {
            input := generateTestData(size)
            
            b.ResetTimer()
            b.ReportAllocs()
            
            for i := 0; i < b.N; i++ {
                FunctionName(input)
            }
        })
    }
}
```

### Different Configurations
```go
func BenchmarkPipeline_Configurations(b *testing.B) {
    configs := []struct {
        name       string
        bufferSize int
        workers    int
    }{
        {"SmallBuffer_2Workers", 100, 2},
        {"LargeBuffer_4Workers", 1000, 4},
        {"LargeBuffer_8Workers", 1000, 8},
    }
    
    for _, config := range configs {
        b.Run(config.name, func(b *testing.B) {
            benchmarkPipelineWithConfig(b, config.bufferSize, config.workers)
        })
    }
}
```

## Pipeline-Specific Benchmarks

### DataStream Operations
```go
func BenchmarkDataStream_Operations(b *testing.B) {
    // Setup DataStream
    ctx := context.Background()
    inputChan := make(chan int, 1000)
    errChan := make(chan error, 10)
    
    ds := datastreams.New(ctx, inputChan, errChan)
    
    // Benchmark different operations
    b.Run("Filter", func(b *testing.B) {
        b.ResetTimer()
        for i := 0; i < b.N; i++ {
            ds.Filter(func(v int) (bool, error) {
                return v%2 == 0, nil
            })
        }
    })
    
    b.Run("Map", func(b *testing.B) {
        b.ResetTimer()
        for i := 0; i < b.N; i++ {
            ds.Map(func(v int) (string, error) {
                return fmt.Sprintf("value_%d", v), nil
            })
        }
    })
}
```

### Window Operations
```go
func BenchmarkWindowing(b *testing.B) {
    // Setup window factory
    factory := windower.NewTumblingFactory[Data](windower.TumblingWindowConfig{
        Size: 100,
    })
    
    // Generate test data
    testData := generateTestData(10000)
    
    b.Run("TumblingWindow", func(b *testing.B) {
        for i := 0; i < b.N; i++ {
            // Create and process windows
            processWindows(testData, factory)
        }
    })
}
```

## Benchmark Utilities

### Test Data Generation
```go
func generateTestData(count int) []TestItem {
    data := make([]TestItem, count)
    for i := 0; i < count; i++ {
        data[i] = TestItem{
            ID:    i,
            Value: fmt.Sprintf("item_%d", i),
            Data:  make([]byte, 100), // Simulate data payload
        }
    }
    return data
}

func generateRandomData(count int) []TestItem {
    data := make([]TestItem, count)
    for i := 0; i < count; i++ {
        data[i] = TestItem{
            ID:    rand.Intn(1000),
            Value: fmt.Sprintf("random_%d", rand.Intn(10000)),
            Data:  make([]byte, rand.Intn(200)+50),
        }
    }
    return data
}
```

### Benchmark Helpers
```go
func benchmarkPipelineWithConfig(b *testing.B, bufferSize, workers int) {
    // Setup pipeline with specific configuration
    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
    defer cancel()
    
    errChan := make(chan error, 100)
    defer close(errChan)
    
    // Create pipeline with config
    source := sources.FromArray(generateTestData(1000))
    pipeline := pipelines.New[Data, Result](ctx, source, errChan)
    
    b.ResetTimer()
    b.ReportAllocs()
    
    // Run pipeline
    result := pipeline.Start(func(ds datastreams.DataStream[Data]) datastreams.DataStream[Result] {
        return ds.FanOut(datastreams.Params{Num: workers}).
            Run(processFunc, datastreams.Params{BufferSize: bufferSize})
    })
    
    // Consume results
    for range result.Out() {
        // Process results
    }
}
```

## Performance Analysis

### CPU Profiling
```go
func BenchmarkWithProfiling(b *testing.B) {
    // Enable CPU profiling
    f, err := os.Create("cpu.prof")
    if err != nil {
        b.Fatal(err)
    }
    defer f.Close()
    
    pprof.StartCPUProfile(f)
    defer pprof.StopCPUProfile()
    
    // Run benchmark
    for i := 0; i < b.N; i++ {
        // Operation to profile
    }
}
```

### Memory Profiling
```go
func BenchmarkWithMemoryProfiling(b *testing.B) {
    // Enable memory profiling
    f, err := os.Create("mem.prof")
    if err != nil {
        b.Fatal(err)
    }
    defer f.Close()
    
    // Run benchmark
    for i := 0; i < b.N; i++ {
        // Operation to profile
    }
    
    // Write memory profile
    pprof.WriteHeapProfile(f)
}
```

## Best Practices

1. **Reset Timer**: Use `b.ResetTimer()` after setup code
2. **Report Allocs**: Use `b.ReportAllocs()` to track memory allocations
3. **Meaningful Names**: Use descriptive benchmark names
4. **Realistic Data**: Use realistic test data sizes and distributions
5. **Multiple Runs**: Test different configurations and data sizes
6. **Profile First**: Profile before optimizing
7. **Compare Results**: Compare benchmarks across different implementations

## Common Anti-Patterns

1. **Including Setup**: Don't include setup code in benchmark timing
2. **Unrealistic Data**: Don't use synthetic data that doesn't represent real usage
3. **Ignoring Memory**: Don't ignore memory allocation patterns
4. **Single Configuration**: Don't benchmark only one configuration
5. **No Error Handling**: Don't ignore errors in benchmarks
description:
globs:
alwaysApply: true
---
